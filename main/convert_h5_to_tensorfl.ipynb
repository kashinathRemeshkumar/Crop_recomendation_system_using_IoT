{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\luzif\\AppData\\Local\\Temp\\tmpem1lxt9e\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\luzif\\AppData\\Local\\Temp\\tmpem1lxt9e\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\luzif\\AppData\\Local\\Temp\\tmpem1lxt9e'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 7), dtype=tf.float32, name='input_layer_1')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 22), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2195595485136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2196302987088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2196302986320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2196302983632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2196302985936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2196302986896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Load the Keras model\n",
    "model = tf.keras.models.load_model(\"my_model.h5\")\n",
    "\n",
    "# Convert to TensorFlow Lite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the converted model\n",
    "with open(\"model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected input shape: [1 7]\n",
      "Output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "##testing thhe created tflite file to check if it still works\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "\n",
    "# Allocate tensors\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Prepare input data (ensure the shape matches)\n",
    "input_shape = input_details[0]['shape']  # Example: [1, 7]\n",
    "print(f\"Expected input shape: {input_shape}\")\n",
    "\n",
    "# Ensure input_data matches the expected shape and type\n",
    "input_data = np.array([[30, 25, 31, 26.31270635, 98.62048026, 5.804965067, 208.1181381]], dtype=np.float32)\n",
    "\n",
    "# Verify the input shape\n",
    "if input_data.shape != tuple(input_shape):\n",
    "    print(f\"Error: Expected input shape {input_shape}, but got {input_data.shape}\")\n",
    "else:\n",
    "    # Set the input tensor\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "    # Run the inference\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Get the output tensor\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    print(\"Output:\", output_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert .tflite model to a C++ header file\n",
    "\n",
    "def convert_tflite_to_cpp_header(model_filename, header_filename):\n",
    "    with open(model_filename, \"rb\") as f:\n",
    "        model_data = f.read()\n",
    "\n",
    "    with open(header_filename, \"w\") as header_file:\n",
    "        header_file.write(\"#ifndef MODEL_H\\n\")\n",
    "        header_file.write(\"#define MODEL_H\\n\\n\")\n",
    "        header_file.write(\"const uint8_t g_model[] = {\\n\")\n",
    "\n",
    "        # Write byte values as comma-separated values in the C++ header\n",
    "        for i in range(0, len(model_data), 12): \n",
    "            header_file.write(\"  \")\n",
    "            header_file.write(\", \".join(f\"0x{byte:02x}\" for byte in model_data[i:i+12]))\n",
    "            header_file.write(\",\\n\")\n",
    "\n",
    "        header_file.write(\"};\\n\\n\")\n",
    "        header_file.write(\"#endif  // MODEL_H\\n\")\n",
    "\n",
    "convert_tflite_to_cpp_header('model.tflite', 'crop_rec_model.h')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
